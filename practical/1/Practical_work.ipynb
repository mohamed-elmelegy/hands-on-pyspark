{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MrBC530gmg5"
      },
      "source": [
        "Let's start with your project: \n",
        "\n",
        "Are you a data scientist? \n",
        "\n",
        "I think you are an awesome a data scientist.\n",
        "\n",
        "### **Problem** \n",
        "**Our goal is to create a predictive model that can answer the following question:**\n",
        "\n",
        "**What kind of people had a better chance of surviving?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhBfwzBgioTD"
      },
      "source": [
        "**Data about passengers:**\n",
        "*   Name\n",
        "*   Age\n",
        "*   Gender.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIEH8iZqi-sk"
      },
      "source": [
        "## Install and Import Libraries\n",
        "Let's install PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj1HhvIOY5Yz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDp80mG9jmfU"
      },
      "source": [
        "## Build Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ttzML9fpjE5a"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as Fns\n",
        "\n",
        "spark = SparkSession.builder.appName('practical_1').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiqECDzLj1Mg"
      },
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn-hxNggkTqV"
      },
      "source": [
        "You have two datasets: \n",
        "* Train  \n",
        "* Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8-A8M7QmKDJ"
      },
      "source": [
        "Read two datasets: \n",
        "* Train\n",
        "* Test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mx2qAccBk15y"
      },
      "outputs": [],
      "source": [
        "train_df = spark.read.csv('train.csv', header=True, inferSchema=True)\n",
        "test_df = spark.read.csv('test.csv', header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj2ANTnWmSCq"
      },
      "source": [
        "Let's work with train dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5mWJR30lNs5"
      },
      "source": [
        "**Confirm if this is a dataframe or not:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tEYTePrzk9yl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train type: <class 'pyspark.sql.dataframe.DataFrame'> \n",
            "Test type: <class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train type: {type(train_df)} \\nTest type: {type(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvLJElPrlT4i"
      },
      "source": [
        "**Show 5 rows.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jYwhqvV8lnO0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QIYVxRXlnnw"
      },
      "source": [
        "**Display schema for the dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pcvERiICl1Ep"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = true)\n",
            " |-- Embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmE3Wd80l1S6"
      },
      "source": [
        "**Statistical summary:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cNY0SItol5Mo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
            "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
            "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
            "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
            "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
            "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
            "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
            "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
            "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiFaIEQTl70_"
      },
      "source": [
        "## EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSNPOnP8mw2Q"
      },
      "source": [
        "**Display count for the train dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zrtpG11Fl9HM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "891"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_6nnTfxm9_x"
      },
      "source": [
        "**Can you answer this question:** \n",
        "\n",
        "**How many people survived, and how many didn't survive?** \n",
        "\n",
        "**Please save data in a variable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QDoqPwyomYxA"
      },
      "outputs": [],
      "source": [
        "survived_count = train_df.filter(Fns.col('Survived') == 1).count()\n",
        "not_survived_count = train_df.filter(Fns.col('Survived') == 0).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8DUtZXPn46m"
      },
      "source": [
        "**Display your result:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|Survived|count|\n",
            "+--------+-----+\n",
            "|       1|  342|\n",
            "|       0|  549|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.groupBy('Survived').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0XHAK8ceoCMU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survived Count: 342 | Not Survived Count: 549\n"
          ]
        }
      ],
      "source": [
        "print(f\"Survived Count: {survived_count} | Not Survived Count: {not_survived_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygsg7wQqor9a"
      },
      "source": [
        "**Can you display your answer in ratio form?(Hint: Use \"UDF\" Function. (Hint: Use \"UDF\" Function. This is a hint you can use any method.)**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3uiaN29PoQnf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survive Ratio: 0.6229508196721312\n"
          ]
        }
      ],
      "source": [
        "# survive_ratio_udf = Fns.udf(lambda x: x.filter(\n",
        "#     Fns.col('Survived') == 1).count() / x.filter(Fns.col('Survived') == 0).count())\n",
        "\n",
        "# survive_ratio_udf= lambda x: x.filter(\n",
        "#     Fns.col('Survived') == 1) / x.filter(Fns.col('Survived') == 0)\n",
        "\n",
        "print(f\"Survive Ratio: {survived_count / not_survived_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7Aker_lp1h4"
      },
      "source": [
        "**Can you get the number of males and females?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XllkDlo3ongJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----+\n",
            "|   Sex|count|\n",
            "+------+-----+\n",
            "|female|  314|\n",
            "|  male|  577|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_df.groupBy('Sex').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHFaJ15zqtEV"
      },
      "source": [
        "**1. What is the average number of survivors of each gender?**\n",
        "\n",
        "**2. What is the number of survivors of each gender?**\n",
        "\n",
        "(Hint: Group by the \"sex\" column. This is a hint you can use any method.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NUikH7MUqdKq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|      avg(Survived)|\n",
            "+-------------------+\n",
            "| 0.7420382165605095|\n",
            "|0.18890814558058924|\n",
            "+-------------------+\n",
            "\n",
            "+------+-----+\n",
            "|   Sex|count|\n",
            "+------+-----+\n",
            "|female|  233|\n",
            "|  male|  109|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Average of `Survived` grouped by `Sex`\n",
        "train_df.groupby('Sex').mean().select(Fns.col('avg(Survived)')).show()\n",
        "\n",
        "# Number of `Survived` grouped by `Sex`\n",
        "train_df.filter(Fns.col(\"Survived\") == 1).groupby(['Sex']).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCEdYNdArtRN"
      },
      "source": [
        "**Create temporary view PySpark:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YjlK6HDUqsI5"
      },
      "outputs": [],
      "source": [
        "train_df.createOrReplaceTempView('train_tmp_view')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXNePifnshHr"
      },
      "source": [
        "**How many people survived, and how many didn't survive? By SQL:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0HxfPRTMslqk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+\n",
            "|Survived|count(1)|\n",
            "+--------+--------+\n",
            "|       1|     342|\n",
            "|       0|     549|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT Survived, COUNT(*) FROM train_tmp_view GROUP BY Survived').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVCdY6EasFWV"
      },
      "source": [
        "**Can you display the number of survivors from each gender as a ratio?**\n",
        "\n",
        "(Hint: Group by \"sex\" column. This is a hint you can use any method.)\n",
        "\n",
        "**Can you do this via SQL?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7xQc3pUUr3HF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------------+------------------+\n",
            "|   Sex|GenderSurviveCount|GenderSurviveRatio|\n",
            "+------+------------------+------------------+\n",
            "|female|               233| 68.12865497076023|\n",
            "|  male|               109| 31.87134502923977|\n",
            "+------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('\\\n",
        "        SELECT Sex, COUNT(*) as GenderSurviveCount, COUNT(*) * 100.0 / SUM(COUNT(*)) over () as GenderSurviveRatio \\\n",
        "        FROM train_tmp_view \\\n",
        "        WHERE Survived=1 \\\n",
        "        GROUP BY Sex \\\n",
        "    ').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6QXc5V8uu3Y"
      },
      "source": [
        "**Display a ratio for \"p-class\": SUM(Survived)/count for p-class**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mscs2mDFdFsD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------+------------------+-------------------+\n",
            "|Pclass|TotalSurvive|PClassSurviveRatio| PClassSurviveRatio|\n",
            "+------+------------+------------------+-------------------+\n",
            "|     1|         136|               216| 0.6296296296296297|\n",
            "|     3|         119|               491|0.24236252545824846|\n",
            "|     2|          87|               184|0.47282608695652173|\n",
            "+------+------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('\\\n",
        "        SELECT \\\n",
        "            Pclass, \\\n",
        "            SUM(Survived) as TotalSurvive, \\\n",
        "            COUNT(*) as PClassSurviveRatio, \\\n",
        "            SUM(Survived) / COUNT(*) as PClassSurviveRatio \\\n",
        "        FROM train_tmp_view \\\n",
        "        GROUP BY Pclass \\\n",
        "    ').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX0klxwAvg6J"
      },
      "source": [
        "**Let's take a break and continue after this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ctM9t8atxJl"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CfanZTCt6Wk"
      },
      "source": [
        "**First and foremost, we must merge both the train and test datasets. (Hint: The union function can do this.)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8Nm8S1K0r4uY"
      },
      "outputs": [],
      "source": [
        "both_df = train_df.unionAll(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI7AD8FLz3iO"
      },
      "source": [
        "**Display count:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s_WERAL8wvJa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1329"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "both_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R4Miuy0z_uP"
      },
      "source": [
        "**Can you define the number of null values in each column?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0LMOalKBxhpD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'PassengerId': 0,\n",
              " 'Survived': 0,\n",
              " 'Pclass': 0,\n",
              " 'Name': 0,\n",
              " 'Sex': 0,\n",
              " 'Age': 265,\n",
              " 'SibSp': 0,\n",
              " 'Parch': 0,\n",
              " 'Ticket': 0,\n",
              " 'Fare': 0,\n",
              " 'Cabin': 1021,\n",
              " 'Embarked': 3}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# both_df.select([Fns.count(Fns.when(Fns.isnan(c), c)).alias(c) for c in both_df.columns]).show()\n",
        "na_dict = {col: both_df.filter(both_df[col].isNull()).count() for col in both_df.columns}\n",
        "na_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBX8cJ000aqe"
      },
      "source": [
        "**Create Dataframe for null values**\n",
        "\n",
        "1. Column\n",
        "2. Number of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ITmyUelNxjJM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StructType(List(StructField(Column,StringType,true),StructField(NumberOfMissingValues,IntegerType,true)))\n",
            "+-----------+---------------------+\n",
            "|     Column|NumberOfMissingValues|\n",
            "+-----------+---------------------+\n",
            "|PassengerId|                    0|\n",
            "|   Survived|                    0|\n",
            "|     Pclass|                    0|\n",
            "|       Name|                    0|\n",
            "|        Sex|                    0|\n",
            "|        Age|                  265|\n",
            "|      SibSp|                    0|\n",
            "|      Parch|                    0|\n",
            "|     Ticket|                    0|\n",
            "|       Fare|                    0|\n",
            "|      Cabin|                 1021|\n",
            "|   Embarked|                    3|\n",
            "+-----------+---------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "missing_vals_df = spark.createDataFrame(na_dict.items(), schema=StructType([\n",
        "    StructField(\"Column\", StringType(), True),\n",
        "    StructField(\"NumberOfMissingValues\", IntegerType(), True)\n",
        "]))\n",
        "print(missing_vals_df.schema)\n",
        "missing_vals_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuKrOi5a0-Ma"
      },
      "source": [
        "## Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVQlr9vDy7Y4"
      },
      "source": [
        "**Create Temporary view PySpark:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xs3yeXhGI8rv"
      },
      "outputs": [],
      "source": [
        "both_df.createOrReplaceTempView('both_tmp_view')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txa8NZIO1JaP"
      },
      "source": [
        "**Can you show the \"name\" column from your temporary table?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "m7yXqJoJy35k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|Braund, Mr. Owen ...|\n",
            "|Cumings, Mrs. Joh...|\n",
            "|Heikkinen, Miss. ...|\n",
            "|Futrelle, Mrs. Ja...|\n",
            "|Allen, Mr. Willia...|\n",
            "+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT name FROM both_tmp_view').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F0F9cTZ2Cuz"
      },
      "source": [
        "**Run this code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0kx6OcB-2BBT"
      },
      "outputs": [],
      "source": [
        "both_df = both_df.withColumn('Title',Fns.regexp_extract(Fns.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
        "both_df.createOrReplaceTempView('both_tmp_view')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZeUWS12r59"
      },
      "source": [
        "**Display \"Title\" column and count \"Title\" column:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hGkFMtlp1FAI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------------+\n",
            "|   Title|count(Title)|\n",
            "+--------+------------+\n",
            "|     Don|           1|\n",
            "|    Miss|         257|\n",
            "|Countess|           2|\n",
            "|     Col|           4|\n",
            "|     Rev|           9|\n",
            "|    Lady|           2|\n",
            "|  Master|          56|\n",
            "|     Mme|           1|\n",
            "|    Capt|           2|\n",
            "|      Mr|         786|\n",
            "|      Dr|          11|\n",
            "|     Mrs|         186|\n",
            "|     Sir|           2|\n",
            "|Jonkheer|           2|\n",
            "|    Mlle|           4|\n",
            "|   Major|           3|\n",
            "|      Ms|           1|\n",
            "+--------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT Title, COUNT(Title) FROM both_tmp_view GROUP BY Title').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLBQDKYu4JOa"
      },
      "source": [
        "**We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so create Dictionary and set the value to \"rare\".**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rjnx5l5r2Qaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Dr': 'rare',\n",
              " 'Rev': 'rare',\n",
              " 'Major': 'rare',\n",
              " 'Col': 'rare',\n",
              " 'Mlle': 'rare',\n",
              " 'Capt': 'rare',\n",
              " 'Don': 'rare',\n",
              " 'Jonkheer': 'rare',\n",
              " 'Countess': 'rare',\n",
              " 'Ms': 'rare',\n",
              " 'Sir': 'rare',\n",
              " 'Lady': 'rare',\n",
              " 'Mme': 'rare',\n",
              " 'Mr': 'Mr',\n",
              " 'Mrs': 'Mrs',\n",
              " 'Master': 'Master',\n",
              " 'Miss': 'Miss'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "titles_map = {\n",
        "    'Dr': 'rare', 'Rev': 'rare', 'Major': 'rare', \n",
        "    'Col': 'rare', 'Mlle': 'rare', 'Capt': 'rare', \n",
        "    'Don': 'rare', 'Jonkheer': 'rare', 'Countess': 'rare', \n",
        "    'Ms': 'rare', 'Sir': 'rare', 'Lady': 'rare', \n",
        "    'Mme': 'rare', \n",
        "    'Mr': 'Mr', 'Mrs': 'Mrs', 'Master': 'Master', 'Miss': 'Miss'\n",
        "}\n",
        "titles_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wrE95Cv7Oqh"
      },
      "source": [
        "**Run the function:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HdDbWuDl7Pf4"
      },
      "outputs": [],
      "source": [
        "def impute_title(title):\n",
        "    return titles_map[title]# Title_map is your dictionary. please change this name with your dictionary name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5EQVIhK7a9R"
      },
      "source": [
        "**Apply the function on \"Title\" column using UDF:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rBAiIOn77XFa"
      },
      "outputs": [],
      "source": [
        "impute_title_UDF = Fns.udf(lambda x: impute_title(x))\n",
        "both_df = both_df.withColumn('Title', impute_title_UDF(Fns.col('Title')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8ewllf7kiV"
      },
      "source": [
        "**Display \"Title\" from table and group by \"Title\" column:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "J9sjQb084GU6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "| Title|\n",
            "+------+\n",
            "|  rare|\n",
            "|  Miss|\n",
            "|Master|\n",
            "|    Mr|\n",
            "|   Mrs|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "both_df.createOrReplaceTempView('both_tmp_view')\n",
        "spark.sql('SELECT Title FROM both_tmp_view GROUP BY Title').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H45QNLj9vJp"
      },
      "source": [
        "## **Preprocessing Age**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwRAhumK-u__"
      },
      "source": [
        "**Based on the \"age\" column mean, you will fill in the missing age values:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eXYSVzvl4z63"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "AgeImputer = Imputer(\n",
        "    strategy='mean',\n",
        "    inputCols = ['Age'],\n",
        "    outputCols = ['Age']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLPivde8_GI-"
      },
      "source": [
        "**Fill missing with \"age\" mean:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lBgW8aFD90PA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age Missing Value Count: 0\n"
          ]
        }
      ],
      "source": [
        "both_df = AgeImputer.fit(both_df).transform(both_df)\n",
        "print(f\"Age Missing Value Count: {both_df.filter(both_df['Age'].isNull()).count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsnUz-m_P95"
      },
      "source": [
        "## **Preprocessing Embarked**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbbamcXMSYP"
      },
      "source": [
        "**Select \"Embarked\" column, count them, order by count Desc, and save in grouped_Embarked variable:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "v-lRu5vc_FW7"
      },
      "outputs": [],
      "source": [
        "both_df.createOrReplaceTempView('both_tmp_view')\n",
        "grouped_Embarked = spark.sql('\\\n",
        "        SELECT Embarked, COUNT(*) as EmbarkedCount\\\n",
        "        FROM both_tmp_view \\\n",
        "        GROUP BY Embarked \\\n",
        "        ORDER BY COUNT(*) DESC\\\n",
        "    ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qf5u2IOQrx"
      },
      "source": [
        "**Show \"groupped_Embarked\" your variable:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jSFNDTNg_erb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|Embarked|EmbarkedCount|\n",
            "+--------+-------------+\n",
            "|       S|          965|\n",
            "|       C|          253|\n",
            "|       Q|          111|\n",
            "+--------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "grouped_Embarked.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQWYgKBMrbp"
      },
      "source": [
        "**Get max of groupped_Embarked:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZLYj4F7E_iqb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(Embarked='S', EmbarkedCount=965)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grouped_Embarked.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8vhoEs8N2w_"
      },
      "source": [
        "**Fill missing values with max 'S' of grouped_Embarked:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LdzQCRud_mAa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Title|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|   Mr|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|  Mrs|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S| Miss|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|  Mrs|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|   Mr|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "both_df = both_df.fillna({'Embarked':grouped_Embarked.first()['Embarked']})\n",
        "both_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEcdV5Vb_qR_"
      },
      "source": [
        "## **Preprocessing Cabin**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BQzPs7tqhpA"
      },
      "source": [
        "**Replace \"cabin\" column with first char from the string:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4b6L5pK0_nQz"
      },
      "outputs": [],
      "source": [
        "both_df = both_df.withColumn(\"Cabin\", Fns.substring('Cabin', 0, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H8XshnYj4k2"
      },
      "source": [
        "**Show the result:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "gJUQwnG1Oj2U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Title|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|   Mr|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|    C|       C|  Mrs|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S| Miss|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|    C|       S|  Mrs|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|   Mr|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "both_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSDsWsUj9Im"
      },
      "source": [
        "**Create the temporary view:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "MR7CXTY7_tMJ"
      },
      "outputs": [],
      "source": [
        "both_df.createOrReplaceTempView('both_tmp_view')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7lfQFkrLlN"
      },
      "source": [
        "**Select \"Cabin\" column, count \"Cabin\" column, Group by \"Cabin\" column, Order By count DESC**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "A0tZG_mvrKXv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|Cabin|count(1)|\n",
            "+-----+--------+\n",
            "| null|    1021|\n",
            "|    C|      82|\n",
            "|    B|      77|\n",
            "|    D|      52|\n",
            "|    E|      51|\n",
            "|    A|      23|\n",
            "|    F|      18|\n",
            "|    G|       4|\n",
            "|    T|       1|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT Cabin, COUNT(*) FROM both_tmp_view GROUP BY Cabin ORDER BY COUNT(*) DESC').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR6j0LOsB4y"
      },
      "source": [
        "**Fill missing values with \"U\":**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mwq5CHEz_up_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Title|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|    U|       S|   Mr|\n",
            "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|    C|       C|  Mrs|\n",
            "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|    U|       S| Miss|\n",
            "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|    C|       S|  Mrs|\n",
            "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|    U|       S|   Mr|\n",
            "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "both_df = both_df.fillna({'Cabin':'U'})\n",
        "both_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRnhA_5-0Hi4"
      },
      "source": [
        "**StringIndexer: A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIKlOX71GQ-"
      },
      "source": [
        "**StringIndexer(inputCol=None, outputCol=None)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- PassengerId: integer (nullable = true)\n",
            " |-- Survived: integer (nullable = true)\n",
            " |-- Pclass: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- SibSp: integer (nullable = true)\n",
            " |-- Parch: integer (nullable = true)\n",
            " |-- Ticket: string (nullable = true)\n",
            " |-- Fare: double (nullable = true)\n",
            " |-- Cabin: string (nullable = false)\n",
            " |-- Embarked: string (nullable = false)\n",
            " |-- Title: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "both_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_drop_cols = ['PassengerId', 'Name', 'Ticket']\n",
        "cat_cols = ['Sex', 'Cabin', 'Embarked', 'Title']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "odhuI2EHKuCm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Convert string to number using StringIndexer\n",
        "category_indexers = []\n",
        "for col in cat_cols:\n",
        "    category_indexers.append(StringIndexer(inputCol=col, outputCol=f\"{col}_index\"))\n",
        "    # Fits a model to the input dataset with optional parameters.\n",
        "    # both_df = category_indexer.fit(both_df).transform(both_df)\n",
        "\n",
        "# both_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DECL9yCK3JZ"
      },
      "source": [
        "**OneHotEncoder(inputCols=None, outputCols=None)**\n",
        "\n",
        "A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "tiAjDEy1LBhz"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "# OneHot Encoding\n",
        "oh_encoders = []\n",
        "for col in cat_cols:\n",
        "    oh_encoders.append(OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_oh_encoded\"))\n",
        "    # Fits a model to the input dataset with optional parameters.\n",
        "    # both_df = oh_encoders.fit(both_df).transform(both_df)\n",
        "\n",
        "# both_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pclass',\n",
              " 'Parch',\n",
              " 'Age',\n",
              " 'Fare',\n",
              " 'SibSp',\n",
              " 'Sex_oh_encoded',\n",
              " 'Cabin_oh_encoded',\n",
              " 'Embarked_oh_encoded',\n",
              " 'Title_oh_encoded']"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create list for features names\n",
        "features_list = list(set(both_df.columns) - (set(cat_cols + to_drop_cols + ['Survived'])))\n",
        "for col in cat_cols:\n",
        "    features_list.append(f\"{col}_oh_encoded\")\n",
        "\n",
        "features_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FsiLsd9452v"
      },
      "source": [
        "**VectorAssembler: VectorAssembler(*, inputCols=None, outputCol=None). A feature transformer that merges multiple columns into a vector column.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "BuytKk0hLE6p"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols = features_list,\n",
        "    outputCol = \"features\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU8DeZfh7JIo"
      },
      "source": [
        "**Use randomSplit function and split data to x_train, and X_test with 80% and 20% Consecutive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "8C11xf1iAzKp"
      },
      "outputs": [],
      "source": [
        "# Split data into train & test\n",
        "(x_train, x_test) = both_df.randomSplit([.8, .2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0c_Hf_b0R12"
      },
      "source": [
        "**Pipeline: ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJQvmFai72O7"
      },
      "source": [
        "**Build RandomForestClassifier model and use pipeline to fit and transform then display \"prediction, Survived, features\" columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "YnpmZqlTLPGq"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rfc = RandomForestClassifier(labelCol='Survived', featuresCol='features')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+----------+\n",
            "|            features|Survived|prediction|\n",
            "+--------------------+--------+----------+\n",
            "|(20,[0,2,3,4,5,6,...|       0|       0.0|\n",
            "|(20,[0,2,3,4,7,15...|       1|       1.0|\n",
            "|(20,[0,2,3,6,14,1...|       1|       1.0|\n",
            "+--------------------+--------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# create pipeline\n",
        "stages = category_indexers + oh_encoders + [assembler] + [rfc]\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# train RFC model\n",
        "pipeline_model = pipeline.fit(x_train)\n",
        "\n",
        "# get train predictions\n",
        "train_predictions = pipeline_model.transform(x_train)\n",
        "train_predictions.select('features', 'Survived', 'prediction').show(3)\n",
        "\n",
        "# get test predictions \n",
        "test_predictions = pipeline_model.transform(x_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSXEI8-r8bKY"
      },
      "source": [
        "**Use MulticlassClassificationEvaluator and set the \"labelCol\" to \"Survived\",  \"predictionCol\" to \"prediction\", \"metricName\" to \"accuracy\"** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Rl0UAKCaBDO-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.846\n",
            "Testing Accuracy: 0.849\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "rfc_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# training Accuracy\n",
        "print(\"Training Accuracy: %.3f\" % rfc_eval.evaluate(train_predictions))\n",
        "\n",
        "# testing Accuracy\n",
        "print(\"Testing Accuracy: %.3f\" % rfc_eval.evaluate(test_predictions))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Practical_work.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "190d5a7d1ffca2a51f17da0d8bcbc5e2c402a41ac372e7ab2f2f850328a57f31"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
